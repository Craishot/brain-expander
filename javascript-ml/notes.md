# Machine Learning With Javascript

## Section 1: What is Machine Learning

### The Rainfall Problem
* We want to predict the amount of flood damage that occurs based on a given amount of rainfall
* In this case we care about two pieces of data, the amount of rainfall on any given year and the amount of flood damage that occurs on any given year
  * With this, the amount of rainfall for each year comprises our `features` and the amount of flood damage comprises our `labels`
* Because we are predicting an outcome from a continuous set, we want to make use of a `regression` algorithm to determine a correlation between our features and labels
  * Based on our chosen algorithm, the hope is that the program discovers a relationship between our features and labels

### A Machine Learning Problem Solving Process
1. Identify the data that is relevant to the problem
   * Independent variables are also knows as `features`
   * Dependent variables are also known as `labels`
2. Assemble a set of data related ot the problem you are trying to solve
   * Data will not always be handed to you on a silver platter; groundwork research may need to be done
3. Decide on the type of output you are predicting
   * There are two common types of data that we may try to predict:
    1. **Classification**: The value of our labels belong to a discrete set
       * This means we are predicting a value from a discrete (limited) set
    2. **Regression**: The value of our labels belong to a continuous set
       * This means we are predicting a value from a large range of values, i.e. the outcome of the prediction could be any value from a continuous set
4. Based on the type of output, pick an algorithm that will determine a correlation between your features and labels
5. Use model generated by algorithm to make a prediction

### Important Takeaways From the Problem Solving Process Above
* **Features** are categories of data points that affect the value of a **label**
* Datasets almost always require cleanup or formatting
* **Regression** used with continuous values, **classification** used with discrete values
* Many, many different algorithms exist, each with pros and cons
* Models relate the value of **features** to the value of **labels**

### The Plinko Problem
* **Goal**: Given some data about where a ball is dropped from, can we predict what bucket it will end up in?
* Relevant Data:
  * Drop position (in pixels) -> `feature`
  * The bucket the ball lands in -> `label`
  * The bounciness of the ball -> `feature`
  * The size of the ball -> `feature`
* Based on our determined relevant data, we have three features and one label, where the drop position, ball bounciness, and ball size affect the bucket the ball will land in.
* In Javascript where are two distinct ways in which we can record datasets:
  1. Array of objects
  2. Array of arrays
    * The array of arrays approach is initially more confusing but once we're used to it, it's a lot easier to make use of
    * With this approach we have to keep track of the meaning of each index of the inner arrays
* Based on that there is a discrete set of buckets available for the balls to drop in, we are working with a classification problem, and thus need to make use of a classification algorithm
* Stephen picked the classification algorithm we're going to use for us, and so we're using the **K-Nearest Neighbor (knn)** algorithm (because it makes intuitive sense to us ML beginners)

## Section 2: Algorithm Overview

### K-Nearest Neighbor (KNN) Algorithm
* The gist of K-Nearest Neighbor is "Birds of the same feather fly together" 
  * From this, in the context of our Plinko problem, this would mean that if our data shows that balls with a range of certain values for each feature fall into a specific bucket, we can predict outcomes based on those values being close to values in the range.
* K-Nearest Neighbor (with one independent variable) Steps (this doesn't need to make sense yet)
  1. Drop a ball a bunch of times all around the board and record which bucket it goes into
  2. For each observation, subtract the drop point from 300px (for predicting bucket at 300px drop) and take absolute value
  3. Sort the results from least to greatest
  4. Look at the 'k' top records. What was the most common bucket?
  5. Whichever bucket came up most frequently is the one ours will probably go into
  * Note that this implementation will only work for 1 feature (independent variable). Initially, we are going to implement KNN using only the drop position feature
* It's important to note that when we are making these predictions, that it is common for us to get bad results on our first attempt at running the prediction and do something similar to the following steps:
  1. Adjust the parameters of the analysis
  2. Add more features to explain the analysis
  3. Change the prediction point
  4. Accept that maybe there isn't a good correlation
* It's important to determine a way of determining accuracy in our test results for any value of `k` and any set of `feature` values

### Finding an Ideal K
1. Record a bunch of data
2. Split that data into a training set and a test set
   * When splitting out data into a test set and a training set, we need to first shuffle our data to avoid getting a lot of inaccurate results. (In the case of our problem, we don't want one set of the data to be the left side of the board and the other to be the right side of the board because then we would be using drop points on one side of the board to predict results for the other side of the board which would lead to some very strange results)
3. For each test record, run KNN using the training data
4. Does the results of KNN equal the test record bucket?
* When determining a value for `k`, it is usually a good idea to just run the KNN algorithm a bunch of times and determine a `k` that lands in some sort of sweet spot

### Multi-Dimensional KNN
* In order to implement multidimensional KNN in our problem, we are going to modify our distance function to use the 3D Pythagorean Theorem (`D = (A ** 2 + B ** 2 + C ** 2) ** 0.5`)
* When constructing methods and functions for use in our KNN algorithm, it is extremely important that we do not modify the original array in any way, such as by using the `pop` method as we would then be losing access to data that we may need when referencing that variable later.
* When using multiple features in our KNN algorithm, it is important to consider the magnitude to which the feature values contribute to the distance calculation.
  * In the case of our Plinko problem, the drop position has a much larger impact on the results of the distance calculation than the ball bounciness, which results in the addition of this feature being essentially useless. This issue can be solved using standardization and normalization.

### Feature Normalization
* We apply normalization to one feature at a time
* With normalization we proportionally scale all of our values for a particular feature on a scale of 0 to 1
  * This allows us to avoid any issue with the magnitude in which the values of each feature contribute to our distance calculation
* In the case of our Plinko problems, we can use the minMax method to normalize our individual pieces of data
  * The `minMax` method is as follows:
  ```js
  const normalizedData = (featureValue - minFeatureValue) / (maxFeatureValue - minFeatureValue)
  ```

### Feature Selection with KNN
* After normalizing our data, we found that weighting drop position, ball bounciness and ball size was a bad idea as it led to an accuracy of less than 10% in most cases
  * This is because, changes to the drop position result in predictable changes to our output, but changes to the ball bounciness results in changes to our output, but NOT predictably
    * Essentially, it's good to know the ball bounciness effects the output, but it's not a good idea to use it as a feature to predict the outcome of a specific drop because it changes the output in an unpredictable way.
* To get around this, we need to carefully perform **feature selection** to decide which features to include in our analysis
  * We can either do a "gut check" intuitive way of determining what features to use in our predictions, or we can do an objective analysis and use some sort of algorithm to determine what features we should use in our predictions